from infre.tools import apriori

# TODO: sentiment analysis - by product

from infre.models import GSB
import numpy as np
from numpy import zeros
from infre.helpers.functions import prune_graph
from sklearn.metrics.pairwise import cosine_similarity
from infre.metrics import precision_recall
from time import time

class ConGSB(GSB):
    def __init__(self, collection, clusters, cond={}):
        
        super().__init__(collection)

        self.model = self._model()
        
        self.graph, self.embeddings, self.prune = prune_graph(self.graph, collection, n_clstrs=clusters, condition=cond)
        
        # self._nwk() # ??? that was not here andd _cnwk was calculated based on the gsb nwk!! (with no prune)
        self._cnwk()


    def _model(self): return __class__.__name__
    

    def expand_q(self, query, k):

        inv_index = self.collection.inverted_index

        # Get collection indices for each term in query
        indices = [inv_index[term]['id'] for term in query if term in inv_index]

        # if query term not in collection, instantly append in expansion terms
        # Since there is no embedding calcualte for it
        expansion_terms = [term for term in query if term not in inv_index]

        # Get query embeddings and drop label (cluster id)
        query_embeddings = self.embeddings.iloc[indices, :-1].values
        
        # Calculate query point in embedding space by taking the mean
        qv = np.mean(query_embeddings , axis=0)

        # Compute similarity between query and the collection terms in that space
        similarities = cosine_similarity([qv], self.embeddings.iloc[:, :-1].values)[0]
        
        # Get the indices of the top k similarities in descending order
        # In our case, double the query
        top_k_indices = np.argsort(-similarities)[:k]

        col_terms = list(inv_index.keys())
        # Add the expansion terms to the list preventing duplicates
        expansion_terms.extend([col_terms[k_ind] for k_ind in top_k_indices 
                                                    if col_terms[k_ind] not in query])
        
        return expansion_terms

    
    def fit_evaluate(self, queries, relevants):

        # inverted index of collection documents
        inv_index = self.collection.inverted_index

        # for each query
        for i, (query, rel) in enumerate(zip(queries, relevants), start=1):

            ################# QUERY EXPANSION ###################
            # k = int(len(query)/2)+1 if len(query) > 12 else len(query)
            k = len(query)
            query += self.expand_q(query, k)

            # apply apriori to find frequent termsets
            freq_termsets = apriori(query, inv_index, min_freq=1)
            
            print(f"Query {i}/{len(queries)}: len = {len(query)}, frequent = {len(freq_termsets)}")

            # vectorized query generated by apriori
            idf_vec = self.query2vec(freq_termsets) # (1 X len(termsets)) vector
            
            # vectorized documents generated by apriori query
            tsf_ij = self.termsets2vec(freq_termsets) # (len(termsets) X N) matrix
            
            # model adjusting weight
            weights = self._model_func(freq_termsets)
            
            # document - termset matrix - model balance weight
            dtsm = self._vectorizer(tsf_ij, idf_vec, weights)
            
            # cosine similarity between query and every document
            qd_sims = self.qd_similarities(idf_vec, dtsm)
            
            # rank them in desc order
            retrieved_docs = self.rank_documents(qd_sims)

            # precision | recall of ranking
            pre, rec = precision_recall(retrieved_docs.keys(), rel)
            
            print(f"=> Query {i}/{100}, precision = {pre:.3f}, recall = {rec:.3f}")
        
            self.precision.append(round(pre, 3))
            self.recall.append(round(rec, 3))

        return np.array(self.precision), np.array(self.recall)

    # .272
    def _cnwk(self):
        # Dictionary to store computed _cnwk values for each cluster
        cluster_cnwk = {}
        
        for term in self.collection.inverted_index:
            # Get the cluster of the current term
            cluster = self.graph.nodes[term]['cluster']
            
            # Check if _cnwk value for the cluster has been computed before
            if cluster not in cluster_cnwk:
                _cnwk = 0
                cluster_size = 0
                
                # Iterate over nodes and compute _cnwk for the cluster
                for node, attrs in self.graph.nodes(data=True):
                    if attrs['cluster'] == cluster:
                        cluster_size += 1
                        _cnwk += self.collection.inverted_index[node]['nwk']
                
                # Compute average _cnwk value for the cluster
                cluster_cnwk[cluster] = round(_cnwk / cluster_size, 3)
            
            # Assign the computed _cnwk value to the current term
            self.collection.inverted_index[term]['cnwk'] = cluster_cnwk[cluster]

        return


    def _model_func(self, termsets): 

        inv_index = self.collection.inverted_index
        tns = zeros(len(termsets), dtype=float)

        for i, termset in enumerate(termsets):
            tw = 1
            for term in termset:
                if term in inv_index:
                    # get the cnw of term k and mulitply it to total
                    tw *= inv_index[term]['cnwk']
            tns[i] = tw

        return tns