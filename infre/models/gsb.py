from networkx import Graph, set_node_attributes, get_node_attributes, to_numpy_array, is_empty
from numpy import array, dot, fill_diagonal, zeros
from networkx.readwrite import json_graph
from math import log2
from json import dumps, load
from os.path import join
from pickle import load
import infre.helpers.utilities as utl
from infre.tools import apriori

from infre.models import BaseIRModel
from infre import retrieval


class GSB(BaseIRModel):
    def __init__(self, collection):
        super().__init__(collection)
        
        # empty graph to be filled by union
        self.graph = self.union_graph()

        # NW Weight of GSB
        self.nwk = self._nwk()

        self.tns = []


    def fit(self, queries, mf=1):

        # inverted index of collection documents
        inv_index = self.collection.inverted_index

        # for each query
        for i, query in enumerate(queries, start=1):
        
            print(f"=> Query {i} of {len(queries)}")

            # apply apriori to find frequent termsets
            freq_termsets = apriori(query, inv_index, min_freq=mf)
            
            print(f"Query length: {len(query)} | Frequent Termsets: {len(freq_termsets)}")
            
            # tns_i weight for each termset produced by query
            self.tns.append(self._tnsi(freq_termsets))
           
            # vectorized query generated by apriori
            idf_vec = self.query2vec(freq_termsets) # (1 X len(termsets)) vector
            
            # vectorized documents generated by apriori query
            tsf_ij = self.termsets2vec(freq_termsets) # (len(termsets) X N) matrix
            
            # append matrix representation of termset frequency per document
            self._docs2vec.append(tsf_ij)

            # append vector query generated by apriori
            self._q2vec.append(idf_vec)

            # print(f'{(time() - start):.2f} secs.\n') TODO: loadbar
        return self


    def evaluate(self, relevant):
        
        num_of_q = len(self._q2vec)

        # for each query and (dtm, relevant) pair
        for i, (qv, dv, rel) in enumerate(zip(self._q2vec, self._docs2vec, relevant)):
            
            # all the money function
            # document - termset matrix
            dtsm = self.tsf_idf_tns(dv, qv, self.tns[i])

            # cosine similarity between query and every document
            qd_sims = self.qd_similarities(qv, dtsm)

            # rank them in desc order
            retrieved_docs = self.rank_documents(qd_sims)

            # precision | recall of ranking
            pre, rec = retrieval.precision_recall(retrieved_docs.keys(), rel)

            print(f"=> Query {i+1} of {num_of_q}")
            print(f'Precision: {pre:.3f} | Recall: {rec:.3f}')

            self.precision.append(round(pre, 3))
            self.recall.append(round(rec, 3))

        return array(self.precision), array(self.recall)
    

    def model(self): return __class__.__name__
    

    def _tnsi(self, termsets):
        tns = []
        for termset in termsets:
            tw = 1
            for term in termset:
                if term in self.nwk:
                    tw *= self.nwk[term]
            tns += [round(tw, 3)]

        return array(tns)


    def tsf_idf_tns(self, tf_ij, idf, tns):
        return tf_ij * (idf * tns).reshape(-1, 1)


    ##############################################
    ## Creating a complete graph TFi*TFj = Wout ##
    ##############################################
    def doc2adj(self, document):

        # get list of term frequencies
        rows = array(list(document.tf.values()))

        # reshape list to column and row vector
        row = rows.reshape(1, rows.shape[0]).T
        col = rows.reshape(rows.shape[0], 1).T

        # create adjecency matrix by dot product
        adj_matrix = dot(row, col)

        # calculate Win weights (diagonal terms)
        win = [(w * (w + 1) * 0.5) for w in rows]

        # assign weights of each nodes
        fill_diagonal(adj_matrix, win)

        return adj_matrix


    def union_graph(self, kcore=[], kcore_bool=False):

        union = Graph()

        # for every graph document object
        for doc in self.collection.docs:
            terms = list(doc.tf.keys())

            adj_matrix = self.doc2adj(doc)
                
            # iterate through lower triangular matrix
            for i in range(adj_matrix.shape[0]):
                # gain value of importance
                h = 0.06 if terms[i] in kcore and kcore_bool else 1

                for j in range(adj_matrix.shape[1]):
                    if i >= j:
                        if union.has_edge(terms[i], terms[j]):
                            union[terms[i]][terms[j]]['weight'] += (adj_matrix[i][j] * h)
                        else:
                            union.add_edge(terms[i], terms[j], weight=adj_matrix[i][j] * h)

        # in-wards edge weights represent Win
        w_in = {n: union.get_edge_data(n, n)['weight'] for n in union.nodes()}

        # set them as node attr
        set_node_attributes(union, w_in, 'weight')

        # remove in-wards edges
        for n in union.nodes(): union.remove_edge(n, n)

        return union
        
    
    def win(self):
        return get_node_attributes(self.graph, 'weight')


    def wout(self):
        return {node: val for (node, val) in self.graph.degree(weight='weight')}


    def number_of_nbrs(self):
        return {node: val for (node, val) in self.graph.degree()}


    def _nwk(self, a=1, b=10):

        if is_empty(self.graph): 
            self.graph = self.union_graph()
  
        nwk = {}
        Win = self.win()
        Wout = self.wout()
        ngb = self.number_of_nbrs()
        a, b = a, b

        for k in list(Win.keys()):
            f = a * Wout[k] / ((Win[k] + 1) * (ngb[k] + 1))
            s = b / (ngb[k] + 1)
            nwk[k] = round(log2(1 + f) * log2(1 + s), 3)
            # print(f'log(1 + ({a} * {Wout[k]} / (({Win[k]} + 1) * ({ngb[k]} + 1)) ) ) * log(1 + ({b} / ({ngb[k]} + 1))) = {nwk[k]}')

        return nwk
    



    def save_graph_index(self, name='graph_index.json'):
        
        # check if union is created, otherwise auto-create
        if is_empty(self.graph): self.graph = self.union_graph()

        # define path to store index
        path = join(self.path['index_path'], name)

        # format data to store
        graph_index = json_graph.adjacency_data(self.graph)

        try:
            # store via the help of json dump
            with open(path, "w") as gf:
                gf.write(dumps(graph_index, cls=utl.NpEncoder))
        
        # if directory does not exist
        except FileNotFoundError:
                # create directory 
                self.create_model_directory()

                # call method recursively to complete the job
                self.save_graph_index()
        finally: # if fails again, reteurn object
            return self


    def load_graph(self, name='graph_index.json'):

        # path to find stored graph index
        path = join(self.path['index_path'], name)

        try:
            # open file and read as dict
            with open(path) as gf: js_graph = load(gf)
        
        except FileNotFoundError:
            raise('There is no such file to load collection.')

        self.graph = json_graph.adjacency_graph(js_graph)

        return self.graph