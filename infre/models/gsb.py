from networkx import Graph, set_node_attributes, get_node_attributes, to_numpy_array, is_empty
from numpy import array, dot, fill_diagonal, zeros
from networkx.readwrite import json_graph
from math import log2, log
from json import dumps, load
from pickle import load, dump
import infre.helpers.utilities as utl
from infre.tools import apriori
from os.path import join, exists
from os import makedirs, getcwd
from bz2 import BZ2File

from infre.models import BaseIRModel
from infre.metrics import precision_recall


class GSB(BaseIRModel):
    def __init__(self, collection):
        super().__init__(collection)
        
        # model name
        self.model = self._model()

        # empty graph to be filled by union
        self.graph = self.union_graph()

        # NW Weight of GSB
        self.nwk = self._nwk()

        self.tns = []


    def fit(self, queries, mf=1):

        # inverted index of collection documents
        inv_index = self.collection.inverted_index

        # for each query
        for i, query in enumerate(queries, start=1):

            # apply apriori to find frequent termsets
            freq_termsets = apriori(query, inv_index, min_freq=mf)
            
            print(f"Query {i}/{len(queries)}: len = {len(query)}, frequent = {len(freq_termsets)}")
            
            # tns_i weight for each termset produced by query
            self.tns.append(self._tnsi(freq_termsets))
           
            # vectorized query generated by apriori
            idf_vec = self.query2vec(freq_termsets) # (1 X len(termsets)) vector
            
            # vectorized documents generated by apriori query
            tsf_ij = self.termsets2vec(freq_termsets) # (len(termsets) X N) matrix
            
            # append matrix representation of termset frequency per document
            self._docs2vec.append(tsf_ij)

            # append vector query generated by apriori
            self._q2vec.append(idf_vec)

            # print(f'{(time() - start):.2f} secs.\n') TODO: loadbar
        return self


    def evaluate(self, relevant):
        
        num_of_q = len(self._q2vec)

        # for each query and (dtm, relevant) pair
        for i, (qv, dv, rel) in enumerate(zip(self._q2vec, self._docs2vec, relevant)):
            
            # all the money function
            # document - termset matrix
            dtsm = self.tsf_idf_tns(dv, qv, self.tns[i])

            # cosine similarity between query and every document
            qd_sims = self.qd_similarities(qv, dtsm)

            # rank them in desc order
            retrieved_docs = self.rank_documents(qd_sims)

            # precision | recall of ranking
            pre, rec = precision_recall(retrieved_docs.keys(), rel)

            print(f"=> Query {i+1}/{num_of_q}, precision = {pre:.3f}, recall = {rec:.3f}")

            self.precision.append(round(pre, 3))
            self.recall.append(round(rec, 3))

        return array(self.precision), array(self.recall)
    

    def _model(self): return __class__.__name__
    
    # based on GSB model tns[i] is a weight represented by
    # the product of all tnw[i] of each term contained in a termset
    def _tnsi(self, termsets):

        # initialize arr
        tns = zeros(len(termsets), dtype=float)

        for i, termset in enumerate(termsets):
            tw = 1
            for term in termset:
                if term in self.nwk:
                    # get the nw weight of term k and mulitply it to total
                    tw *= self.nwk[term]
            tns[i] = round(tw, 3)

        return tns


    def tsf_idf_tns(self, tsf_ij, idf, tns):
        return tsf_ij * (idf * tns).reshape(-1, 1)


    ##############################################
    ## Creating a complete graph TFi*TFj = Wout ##
    ##############################################
    def doc2adj(self, document):

        # get list of term frequencies
        rows = array(list(document.tf.values())).reshape((-1, 1))

        # create adjecency matrix by dot product
        adj_matrix = dot(rows, rows.T)

        # calculate Win weights (diagonal terms)
        win = [(w * (w + 1) * 0.5) for w in rows]

        # assign weights of each nodes
        fill_diagonal(adj_matrix, win)

        return adj_matrix


    def union_graph(self, kcore=[], kcore_bool=False):

        union = Graph() # empty graph 

        # for every graph document object
        for doc in self.collection.docs:

            # get terms of each document
            terms = list(doc.tf.keys())

            # create it's adjecency matrix based on Makris alg
            adj_matrix = self.doc2adj(doc)
                
            for i in range(adj_matrix.shape[0]):
                # gain value of importance
                h = 0.06 if terms[i] in kcore and kcore_bool else 1

                for j in range(adj_matrix.shape[1]):
                    # iterate through lower triangular matrix
                    if i >= j:
                        if union.has_edge(terms[i], terms[j]):
                            union[terms[i]][terms[j]]['weight'] += (adj_matrix[i][j] * h)
                        else:
                            union.add_edge(terms[i], terms[j], weight=adj_matrix[i][j] * h)

        # in-wards edge weights represent Win
        w_in = {n: union.get_edge_data(n, n)['weight'] for n in union.nodes()}

        # set them as node attr
        set_node_attributes(union, w_in, 'weight')

        # remove self edges
        for n in union.nodes(): union.remove_edge(n, n)
    
        from networkx import to_numpy_array
        # from sklearn.cluster import SpectralClustering
        from infre.tools import SpectralClustering

        # Cluster the nodes using spectral clustering
        sc = SpectralClustering(n_clusters=50, affinity='precomputed', assign_labels='discretize')
        
        adj_matrix = to_numpy_array(union)
        labels, _embeddings = sc.fit_predict(adj_matrix)

        # Remove edges between nodes in different clusters
        for u, v in union.edges():
            c, w = self.collection.inverted_index[u]['id'], self.collection.inverted_index[v]['id']

            if labels[c] != labels[w]:
                # union.add_node(u, cluster=labels[c])
                # union.add_node(v, cluster=labels[w])
                union.remove_edge(u, v)

        for node in union.nodes():
            idx = self.collection.inverted_index[node]['id']
            union.add_node(node, cluster=labels[idx])
        # import matplotlib.pyplot as plt
        # import numpy as np
        
        # dim reduction with SVD
        # from sklearn.decomposition  import PCA
        # embSvd = PCA(2).fit_transform(_embeddings)

        # for i in np.unique(labels):
        #     plt.scatter(embSvd[labels == i, 0], embSvd[labels == i, 1], label=i)
        # plt.show()
        # print("Dellta average")
        # print(sum(value for _, value in union.degree()) / union.number_of_nodes())

        return union
        
    
    def _win(self):
        return get_node_attributes(self.graph, 'weight')


    def _wout(self):
        return {node: val for (node, val) in self.graph.degree(weight='weight')}


    def _number_of_nbrs(self):
        return {node: val for (node, val) in self.graph.degree()}


    def _nwk(self, a=1, b=10):

        if is_empty(self.graph): 
            self.graph = self.union_graph()
  
        nwk = {}
        Win = self._win()
        Wout = self._wout()
        ngb = self._number_of_nbrs()
        a, b = a, b

        for k in list(Win.keys()):
            f = a * Wout[k] / ((Win[k] + 1) * (ngb[k] + 1))
            s = b / (ngb[k] + 1)
            nwk[k] = round(log2(1 + f) * log2(1 + s), 3)
            # print(f'log(1 + ({a} * {Wout[k]} / (({Win[k]} + 1) * ({ngb[k]} + 1)) ) ) * log(1 + ({b} / ({ngb[k]} + 1))) = {nwk[k]}')

        return nwk
    

        # picke model
    def save_model(self, path, name='config.model'):
        
        # define indexes path
        dir = join(getcwd(), path, self.model)
   
        if not exists(dir):
            makedirs(dir)

        path = join(dir, name)

        try:
            with BZ2File(path, 'wb') as config_model:
                dump(self, config_model)

        except FileNotFoundError:
                FileNotFoundError


    # un-picke model
    def load_model(self, **kwargs):

        # define indexes path
        try:
            path = join(getcwd(), kwargs['dir'], self.model, kwargs['name'])
        except KeyError:
            raise KeyError

        try:
            with BZ2File(path, 'rb') as config_model:
                return load(config_model)

        except FileNotFoundError:
                raise FileNotFoundError
        

    def save_graph_index(self, name='graph_index.json'):
        
        # check if union is created, otherwise auto-create
        if is_empty(self.graph): self.graph = self.union_graph()

        # define path to store index
        path = join(self.path['index_path'], name)

        # format data to store
        graph_index = json_graph.adjacency_data(self.graph)

        try:
            # store via the help of json dump
            with open(path, "w") as gf:
                gf.write(dumps(graph_index, cls=utl.NpEncoder))
        
        # if directory does not exist
        except FileNotFoundError:
                # create directory 
                self.create_model_directory()

                # call method recursively to complete the job
                self.save_graph_index()
        finally: # if fails again, reteurn object
            return self



    def load_graph(self, name='graph_index.json'):

        # path to find stored graph index
        path = join(self.path['index_path'], name)

        try:
            # open file and read as dict
            with open(path) as gf: js_graph = load(gf)
        
        except FileNotFoundError:
            raise('There is no such file to load collection.')

        self.graph = json_graph.adjacency_graph(js_graph)

        return self.graph